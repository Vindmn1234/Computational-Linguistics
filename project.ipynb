{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
    "# !unzip wikitext-2-v1.zip -d wikitext-2\n",
    "nltk.download('punkt')\n",
    "def load_wikitext(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "words = load_wikitext('wikitext-2/wikitext-2/wiki.train.tokens')\n",
    "\n",
    "num_characters = len(words)\n",
    "num_words = len(word_tokenize(words))\n",
    "num_lines = words.count('\\n')\n",
    "print(f\"Number of Characters: {num_characters}\")\n",
    "print(f\"Number of Words: {num_words}\")\n",
    "print(f\"Number of Lines: {num_lines}\")\n",
    "\n",
    "words = word_tokenize(words)\n",
    "# Convert words to lower case and filter out non-alpha characters\n",
    "words= [word for word in words if word.isalpha()]\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "split_index = int(0.6 * len(words))\n",
    "training_words = words[:split_index]\n",
    "test_words = words[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create N-grams from the training set\n",
    "bigrams_train = list(ngrams(training_words, 2))\n",
    "trigrams_train = list(ngrams(training_words, 3))\n",
    "fourgrams_train = list(ngrams(training_words, 4))\n",
    "\n",
    "# Frequency counts for N-grams from the training set\n",
    "bigram_freq_train = Counter(bigrams_train)\n",
    "trigram_freq_train = Counter(trigrams_train)\n",
    "fourgram_freq_train = Counter(fourgrams_train)\n",
    "\n",
    "bigrams_test = list(ngrams(test_words, 2))\n",
    "trigrams_test = list(ngrams(test_words, 3))\n",
    "fourgrams_test = list(ngrams(test_words, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_ngram_model(ngram_freq):\n",
    "    # Create a model where for each N-1 gram, we have all possible next words along with their counts\n",
    "    model = defaultdict(Counter)\n",
    "    for ngram in ngram_freq:\n",
    "        prefix, next_word = ngram[:-1], ngram[-1]\n",
    "        model[prefix][next_word] += 1\n",
    "    return model\n",
    "\n",
    "# Build models\n",
    "bigram_model = build_ngram_model(bigrams_train)\n",
    "trigram_model = build_ngram_model(trigrams_train)\n",
    "fourgram_model = build_ngram_model(fourgrams_train)\n",
    "\n",
    "def predict_next_word(model, context):\n",
    "    # Context should be a tuple of N-1 words\n",
    "    if context in model:\n",
    "        # Get the most common next word\n",
    "        most_common_next_word = model[context].most_common(1)\n",
    "        return most_common_next_word[0][0] if most_common_next_word else None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, test_ngrams):\n",
    "    correct_predictions, total_predictions = 0, 0\n",
    "\n",
    "    for ngram in test_ngrams:\n",
    "        context, actual_next_word = ngram[:-1], ngram[-1]\n",
    "        predicted_next_word = predict_next_word(model, context)\n",
    "\n",
    "        if predicted_next_word == actual_next_word:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "\n",
    "    return correct_predictions / total_predictions if total_predictions > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(model, test_ngrams):\n",
    "    log_prob_sum = 0\n",
    "    word_count = 0\n",
    "\n",
    "    for ngram in test_ngrams:\n",
    "        context, actual_next_word = ngram[:-1], ngram[-1]\n",
    "        predicted_probability = model[context][actual_next_word] if actual_next_word in model[context] else model[context]['<UNK>']\n",
    "\n",
    "        # Check if predicted_probability is zero and handle it\n",
    "        if predicted_probability > 0:\n",
    "            log_prob_sum += math.log(predicted_probability)\n",
    "        else:\n",
    "            # Handle zero probability case, e.g., by using a very small probability\n",
    "            log_prob_sum += math.log(1e-10)  # You can adjust this value as needed\n",
    "\n",
    "        word_count += 1\n",
    "\n",
    "    return math.exp(-log_prob_sum / word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_2 = calculate_perplexity(bigram_model, bigrams_test)\n",
    "perplexity_3 = calculate_perplexity(trigram_model, trigrams_test)\n",
    "perplexity_4 = calculate_perplexity(fourgram_model, fourgrams_test)\n",
    "# Calculate accuracies\n",
    "accuracy_bigram = calculate_accuracy(bigram_model, bigrams_test)\n",
    "accuracy_trigram = calculate_accuracy(trigram_model, trigrams_test)\n",
    "accuracy_fourgram = calculate_accuracy(fourgram_model, fourgrams_test)\n",
    "\n",
    "print(f\"2-gram Model Accuracy: {accuracy_bigram * 100:.2f}%\")\n",
    "print(f\"3-gram Model Accuracy: {accuracy_trigram * 100:.2f}%\")\n",
    "print(f\"4-gram Model Accuracy: {accuracy_fourgram * 100:.2f}%\")\n",
    "print(\"perplexity of bigrams:\", perplexity_2)\n",
    "print(\"perplexity of trigrams:\", perplexity_3)\n",
    "print(\"perplexity of fourgrams:\", perplexity_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rare_words(words, threshold=1):\n",
    "    word_freq = Counter(words)\n",
    "    return [word if word_freq[word] > threshold else '<UNK>' for word in words]\n",
    "\n",
    "# Replace rare words in the training data\n",
    "threshold = 1  # This can be adjusted based on your dataset\n",
    "training_words = replace_rare_words(training_words, threshold)\n",
    "\n",
    "\n",
    "def build_ngram_model_smoothing(ngram_freq, vocabulary_size, smoothing=0.01):\n",
    "    model = defaultdict(Counter)\n",
    "    for ngram in ngram_freq:\n",
    "        prefix, next_word = ngram[:-1], ngram[-1]\n",
    "        model[prefix][next_word] += 1\n",
    "\n",
    "    # Apply smoothing\n",
    "    for prefix in model:\n",
    "        for word in model[prefix]:\n",
    "            model[prefix][word] += smoothing\n",
    "        model[prefix]['<UNK>'] = smoothing  # Add smoothing for unknown words\n",
    "\n",
    "    # Adjust counts to probabilities\n",
    "    for prefix in model:\n",
    "        total_count = sum(model[prefix].values())\n",
    "        for word in model[prefix]:\n",
    "            model[prefix][word] /= total_count\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "vocabulary = set(training_words)\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "bigram_model_s = build_ngram_model_smoothing(bigrams_train, vocabulary_size)\n",
    "trigram_model_s = build_ngram_model_smoothing(trigrams_train, vocabulary_size)\n",
    "fourgram_model_s = build_ngram_model_smoothing(fourgrams_train, vocabulary_size)\n",
    "\n",
    "\n",
    "def predict_next_word(model, context):\n",
    "    if context not in model:\n",
    "        return '<UNK>'\n",
    "    most_common_next_word = model[context].most_common(1)\n",
    "    return most_common_next_word[0][0] if most_common_next_word else '<UNK>'\n",
    "\n",
    "\n",
    "def predict_with_unk_handling(model, context):\n",
    "    new_context = tuple(word if word in vocabulary else '<UNK>' for word in context)\n",
    "    return predict_next_word(model, new_context)\n",
    "\n",
    "def calculate_accuracy(model, test_ngrams):\n",
    "    correct_predictions, total_predictions = 0, 0\n",
    "\n",
    "    for ngram in test_ngrams:\n",
    "        context, actual_next_word = ngram[:-1], ngram[-1]\n",
    "        predicted_next_word = predict_with_unk_handling(model, context)\n",
    "\n",
    "        if predicted_next_word == actual_next_word:\n",
    "            correct_predictions += 1\n",
    "        total_predictions += 1\n",
    "\n",
    "    return correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "accuracy_bigram_s = calculate_accuracy(bigram_model_s, bigrams_test)\n",
    "accuracy_trigram_s = calculate_accuracy(trigram_model_s, trigrams_test)\n",
    "accuracy_fourgram_s = calculate_accuracy(fourgram_model_s, fourgrams_test)\n",
    "\n",
    "print(f\"2-gram Model Accuracy with smoothing: {accuracy_bigram_s * 100:.2f}%\")\n",
    "print(f\"3-gram Model Accuracy with smoothing: {accuracy_trigram_s * 100:.2f}%\")\n",
    "print(f\"4-gram Model Accuracy with smoothing: {accuracy_fourgram_s * 100:.2f}%\")\n",
    "\n",
    "perplexity_2s = calculate_perplexity(bigram_model_s, bigrams_test)\n",
    "perplexity_3s = calculate_perplexity(trigram_model_s, trigrams_test)\n",
    "perplexity_4s = calculate_perplexity(fourgram_model_s, fourgrams_test)\n",
    "\n",
    "print(\"perplexity of bigrams with smoothing:\", perplexity_2s)\n",
    "print(\"perplexity of trigrams with smoothing:\", perplexity_3s)\n",
    "print(\"perplexity of fourgrams with smoothing:\", perplexity_4s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_word(word, probability=0.05):\n",
    "    \"\"\"Randomly masks a word with a given probability.\"\"\"\n",
    "    return '<MASK>' if random.random() < probability else word\n",
    "    \n",
    "noisy_words_mask = [mask_word(word.lower()) for word in words if word.isalpha()]\n",
    "split_index = int(0.6 * len(noisy_words_mask))\n",
    "training_words_mask = noisy_words_mask[:split_index]\n",
    "\n",
    "bigrams_train_mask = list(ngrams(training_words_mask, 2))\n",
    "trigrams_train_mask = list(ngrams(training_words_mask, 3))\n",
    "fourgrams_train_mask = list(ngrams(training_words_mask, 4))\n",
    "\n",
    "bigram_model_mask = build_ngram_model(bigrams_train_mask)\n",
    "trigram_model_mask = build_ngram_model(trigrams_train_mask)\n",
    "fourgram_model_mask = build_ngram_model(fourgrams_train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_spelling_mistake(word, probability=0.05):\n",
    "    if random.random() < probability and len(word) > 5:\n",
    "        idx = random.randint(0, len(word) - 2)\n",
    "        return word[:idx] + word[idx+1] + word[idx] + word[idx+2:]\n",
    "    return word\n",
    "\n",
    "words = words.split()\n",
    "    \n",
    "noisy_words_spelling = [introduce_spelling_mistake(word.lower()) for word in words if word.isalpha()]\n",
    "split_index = int(0.6 * len(noisy_words_spelling))\n",
    "training_words_spelling = noisy_words_spelling[:split_index]\n",
    "\n",
    "bigrams_train_spelling = list(ngrams(training_words_spelling, 2))\n",
    "trigrams_train_spelling = list(ngrams(training_words_spelling, 3))\n",
    "fourgrams_train_spelling = list(ngrams(training_words_spelling, 4))\n",
    "\n",
    "bigram_model_spelling = build_ngram_model(bigrams_train_spelling)\n",
    "trigram_model_spelling = build_ngram_model(trigrams_train_spelling)\n",
    "fourgram_model_spelling = build_ngram_model(fourgrams_train_spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_bigram_mask = calculate_accuracy(bigram_model_mask, bigrams_test)\n",
    "accuracy_trigram_mask = calculate_accuracy(trigram_model_mask, trigrams_test)\n",
    "accuracy_fourgram_mask = calculate_accuracy(fourgram_model_mask, fourgrams_test)\n",
    "\n",
    "print(f\"2-gram masked Model Accuracy: {accuracy_bigram_mask * 100:.2f}%\")\n",
    "print(f\"3-gram masked Model Accuracy: {accuracy_trigram_mask * 100:.2f}%\")\n",
    "print(f\"4-gram masked Model Accuracy: {accuracy_fourgram_mask * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_bigram_spelling = calculate_accuracy(bigram_model_spelling, bigrams_test)\n",
    "accuracy_trigram_spelling = calculate_accuracy(trigram_model_spelling, trigrams_test)\n",
    "accuracy_fourgram_spelling = calculate_accuracy(fourgram_model_spelling, fourgrams_test)\n",
    "\n",
    "print(f\"2-gram Model Accuracy include spelling mistakes: {accuracy_bigram_spelling * 100:.2f}%\")\n",
    "print(f\"3-gram Model Accuracy include spelling mistakes: {accuracy_trigram_spelling * 100:.2f}%\")\n",
    "print(f\"4-gram Model Accuracy include spelling mistakes: {accuracy_fourgram_spelling * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_2_mask = calculate_perplexity(bigram_model_mask, bigrams_test)\n",
    "perplexity_3_mask = calculate_perplexity(trigram_model_mask, trigrams_test)\n",
    "perplexity_4_mask = calculate_perplexity(fourgram_model_mask, fourgrams_test)\n",
    "perplexity_2_spelling = calculate_perplexity(bigram_model_spelling, bigrams_test)\n",
    "perplexity_3_spelling = calculate_perplexity(trigram_model_spelling, trigrams_test)\n",
    "perplexity_4_spelling = calculate_perplexity(fourgram_model_spelling, fourgrams_test)\n",
    "print(\"perplexity of bigrams with masks:\", perplexity_2_mask)\n",
    "print(\"perplexity of trigrams with masks:\", perplexity_3_mask)\n",
    "print(\"perplexity of fourgrams with masks:\", perplexity_4_mask)\n",
    "print(\"perplexity of bigrams with spelling mistakes included:\", perplexity_2_spelling)\n",
    "print(\"perplexity of trigrams with spelling mistakes included:\", perplexity_3_spelling)\n",
    "print(\"perplexity of fourgrams with spelling mistakes included:\", perplexity_4_spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy_values = [\n",
    "    round(accuracy_bigram * 100, 2),\n",
    "    round(accuracy_trigram * 100, 2),\n",
    "    round(accuracy_fourgram * 100, 2),\n",
    "    round(accuracy_bigram_s * 100, 2),\n",
    "    round(accuracy_trigram_s * 100, 2),\n",
    "    round(accuracy_fourgram_s * 100, 2),\n",
    "    round(accuracy_bigram_mask * 100, 2),\n",
    "    round(accuracy_trigram_mask * 100, 2),\n",
    "    round(accuracy_fourgram_mask * 100, 2),\n",
    "    round(accuracy_bigram_spelling * 100, 2),\n",
    "    round(accuracy_trigram_spelling * 100, 2),\n",
    "    round(accuracy_fourgram_spelling * 100, 2),\n",
    "]\n",
    "labels = ['Bigram', 'Trigram', 'Fourgram', 'Bigram', 'Trigram', 'Fourgram','Bigram', 'Trigram', 'Fourgram','Bigram', 'Trigram', 'Fourgram']\n",
    "\n",
    "labels_without_smoothing = labels[:3]\n",
    "accuracy_without_smoothing = accuracy_values[:3]\n",
    "labels_with_smoothing = labels[3:6]\n",
    "accuracy_with_smoothing = accuracy_values[3:6]\n",
    "labels_with_mask = labels[6:9]\n",
    "accuracy_with_mask = accuracy_values[6:9]\n",
    "labels_with_spelling = labels[9:]\n",
    "accuracy_with_spelling = accuracy_values[9:]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(16, 5))\n",
    "axes[0].bar(labels_without_smoothing, accuracy_without_smoothing, color=['blue', 'orange', 'green'])\n",
    "axes[0].set_title('Accuracy without Smoothing')\n",
    "axes[0].set_ylabel('Percentage (%)')\n",
    "axes[0].set_ylim(0, max(accuracy_values) + 5)\n",
    "\n",
    "axes[1].bar(labels_with_smoothing, accuracy_with_smoothing, color=['blue', 'orange', 'green'])\n",
    "axes[1].set_title('Accuracy with Smoothing')\n",
    "axes[1].set_ylabel('Percentage (%)')\n",
    "axes[1].set_ylim(0, max(accuracy_values) + 5)\n",
    "\n",
    "axes[2].bar(labels_with_mask, accuracy_with_mask, color=['blue', 'orange', 'green'])\n",
    "axes[2].set_title('Accuracy with mask words')\n",
    "axes[2].set_ylabel('Percentage (%)')\n",
    "axes[2].set_ylim(0, max(accuracy_values) + 5)\n",
    "\n",
    "axes[3].bar(labels_with_spelling, accuracy_with_spelling, color=['blue', 'orange', 'green'])\n",
    "axes[3].set_title('Accuracy with spelling mistakes included')\n",
    "axes[3].set_ylabel('Percentage (%)')\n",
    "axes[3].set_ylim(0, max(accuracy_values) + 5)\n",
    "\n",
    "for ax in axes:\n",
    "    for bar in ax.patches:\n",
    "        ax.annotate(f\"{bar.get_height():.2f}%\",\n",
    "                    (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities_without_smoothing = [perplexity_2 , perplexity_3, perplexity_4]\n",
    "perplexities_with_masks = [perplexity_2_mask , perplexity_3_mask, perplexity_4_mask]\n",
    "perplexities_with_spelling = [perplexity_2_spelling , perplexity_3_spelling, perplexity_4_spelling]\n",
    "perplexities_with_smoothing = [perplexity_2s, perplexity_3s, perplexity_4s]\n",
    "labels = ['Bigram', 'Trigram', 'Fourgram']\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(16, 5), sharey=True)\n",
    "\n",
    "axes[0].bar(labels, perplexities_with_smoothing, color=['blue', 'orange', 'green'])\n",
    "axes[0].set_title('Perplexity with Smoothing (Log Scale)')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_ylabel('Perplexity (Log Scale)')\n",
    "axes[0].set_xlabel('N-Gram Model')\n",
    "\n",
    "axes[1].bar(labels, perplexities_without_smoothing, color=['blue', 'orange', 'green'])\n",
    "axes[1].set_title('Perplexity without Smoothing (Log Scale)')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_xlabel('N-Gram Model')\n",
    "\n",
    "axes[2].bar(labels, perplexities_with_masks, color=['blue', 'orange', 'green'])\n",
    "axes[2].set_title('Perplexity with masks (Log Scale)')\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].set_xlabel('N-Gram Model')\n",
    "\n",
    "axes[3].bar(labels, perplexities_with_spelling, color=['blue', 'orange', 'green'])\n",
    "axes[3].set_title('Perplexity with spelling mistakes included (Log Scale)')\n",
    "axes[3].set_yscale('log')\n",
    "axes[3].set_xlabel('N-Gram Model')\n",
    "\n",
    "min_visible = 10**4\n",
    "for ax in axes:\n",
    "    for bar in ax.patches:\n",
    "        height = bar.get_height()\n",
    "        if height < min_visible:\n",
    "            ax.annotate(f'{height:.2e}',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, min_visible),\n",
    "                        xytext=(0, 3),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=8)\n",
    "        else:\n",
    "            ax.annotate(f'{height:.2e}',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "# Load data from wiki-text\n",
    "def load_wikitext(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "text = load_wikitext('wikitext-2/wikitext-2/wiki.train.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Split sentences into words\n",
    "words = word_tokenize(text)\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "vocab = Counter(words)\n",
    "vocab_size = len(vocab) + 1  # '<pad>'\n",
    "\n",
    "# Add <pad>\n",
    "pad_token = '<pad>'\n",
    "word_to_idx = {word: i for i, (word, _) in enumerate(vocab.items())}\n",
    "word_to_idx[pad_token] = vocab_size - 1  # '<pad>'\n",
    "\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "def words_to_indices(sequence):\n",
    "    return [word_to_idx.get(word, word_to_idx[pad_token]) for word in sequence]\n",
    "\n",
    "\n",
    "def process_sequences(sentences, seq_length):\n",
    "    sequences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [word.lower() for word in words if word.isalpha()]\n",
    "        if len(words) < seq_length:\n",
    "            words.extend([pad_token] * (seq_length - len(words)))\n",
    "        seqs = [words[i:i + seq_length] for i in range(len(words) - seq_length + 1)]\n",
    "        sequences.extend(seqs)\n",
    "    return [words_to_indices(seq) for seq in sequences]\n",
    "\n",
    "# Set sequence length\n",
    "seq_length_train = 10  # Sequence size for training\n",
    "seq_length_val_test = 5  # Sequence size for validation and testing\n",
    "\n",
    "# Generate training sequences\n",
    "train_sequences = process_sequences(sentences, seq_length_train)\n",
    "\n",
    "# Generate validation and testing sequences\n",
    "val_test_sequences = process_sequences(sentences, seq_length_val_test)\n",
    "\n",
    "# Convert ot tensor\n",
    "X_train_seq = torch.tensor([seq[:-1] for seq in train_sequences], dtype=torch.long)\n",
    "y_train_seq = torch.tensor([seq[-1] for seq in train_sequences], dtype=torch.long)\n",
    "\n",
    "X_val_test_seq = torch.tensor([seq[:-1] for seq in val_test_sequences], dtype=torch.long)\n",
    "y_val_test_seq = torch.tensor([seq[-1] for seq in val_test_sequences], dtype=torch.long)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_seq, y_train_seq, test_size=0.3, random_state=42) # Training\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test_seq, y_val_test_seq, test_size=0.5, random_state=42)  # Validation and Testing\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [len(sentence) for sentence in sentences]\n",
    "plt.hist(sentence_lengths, bins=range(min(sentence_lengths), max(sentence_lengths) + 1, 1))\n",
    "plt.title(\"Distribution of Sentence Lengths\")\n",
    "plt.xlabel(\"Length of Sentences (words)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # Embedding\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout) # LSTM\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim) # Layer Normalization\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size) # Fully Conntected Layer\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.layer_norm(lstm_out[:, -1, :])\n",
    "        logits = F.softmax(self.fc1(lstm_out), dim = -1) # Another fully connected layer\n",
    "        logits = self.fc(lstm_out)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, targets in loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, targets)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    return total_loss / len(loader), total_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256 \n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = {\n",
    "    'patience': 3,\n",
    "    'counter': 0,\n",
    "    'best_val_loss': float('inf')\n",
    "}\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, train_perplexities, val_perplexities = [], [], [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    train_acc = evaluate(model, train_loader, criterion)[1]\n",
    "\n",
    "    train_perplexity = np.exp(train_loss)\n",
    "    val_perplexity = np.exp(val_loss)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    train_perplexities.append(train_perplexity)\n",
    "    val_perplexities.append(val_perplexity)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f} - Train Accuracy: {train_acc:.4f} - Train Perplexity: {train_perplexity:.4f} - Val loss: {val_loss:.4f} - Val Accuracy: {val_acc:.4f} - Val Perplexity: {val_perplexity:.4f}')\n",
    "\n",
    "    if val_loss < early_stopping['best_val_loss']:\n",
    "        early_stopping['best_val_loss'] = val_loss\n",
    "        early_stopping['counter'] = 0\n",
    "    else:\n",
    "        early_stopping['counter'] += 1\n",
    "\n",
    "    if early_stopping['counter'] >= early_stopping['patience']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_perplexities, label='Train Perplexity')\n",
    "plt.plot(val_perplexities, label='Val Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Split text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Split sentences into words\n",
    "words = word_tokenize(text)\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "vocab = Counter(words)\n",
    "\n",
    "# Add <pad>\n",
    "pad_token = '<pad>'\n",
    "mask_token = '<mask>'\n",
    "misspell_token = '<unk>'\n",
    "vocab_size = len(vocab) + 3\n",
    "\n",
    "word_to_idx = {word: i for i, (word, _) in enumerate(vocab.items())}\n",
    "word_to_idx[pad_token] = vocab_size - 1  # '<pad>'\n",
    "word_to_idx[mask_token] = vocab_size - 2 # '<mask>'\n",
    "word_to_idx[misspell_token] = vocab_size - 3 # '<misspell>'\n",
    "\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "def words_to_indices(sequence):\n",
    "    return [word_to_idx.get(word, word_to_idx[pad_token]) for word in sequence]\n",
    "\n",
    "\n",
    "def process_sequences(s, seq_length):\n",
    "    sequences = []\n",
    "    for sentence in s:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [word.lower() for word in words if word.isalpha()]\n",
    "        if len(words) < seq_length:\n",
    "            words.extend([pad_token] * (seq_length - len(words)))\n",
    "        seqs = [words[i:i + seq_length] for i in range(len(words) - seq_length + 1)]\n",
    "        sequences.extend(seqs)\n",
    "    return [words_to_indices(seq) for seq in sequences]\n",
    "\n",
    "\n",
    "def modify(sequences, mask_token, misspell_token, mask_prob, misspell_prob):\n",
    "    modified_sequences = []\n",
    "    for seq in sequences:\n",
    "        modified_seq = []\n",
    "        for word in seq:\n",
    "            rand_num = random.random()\n",
    "            if rand_num < (mask_prob + misspell_prob):\n",
    "                if rand_num < mask_prob:\n",
    "                    modified_seq.append(mask_token)\n",
    "                else:\n",
    "                    modified_seq.append(misspell_token)\n",
    "            else:\n",
    "                modified_seq.append(word)\n",
    "        modified_sequences.append(modified_seq)\n",
    "    return modified_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length_train = 5  # Sequence size for training\n",
    "seq_length_val_test = 5  # Sequence size for validation and testing\n",
    "\n",
    "s = int(0.7 * len(sentences))\n",
    "# Generate training sequences\n",
    "train_sequences = process_sequences(sentences[:s], seq_length_train)\n",
    "# New\n",
    "train_sequences = modify(train_sequences, vocab_size - 2, vocab_size - 3, 0.0001, 0.0001)\n",
    "\n",
    "ss = int(0.85 * len(sentences))\n",
    "# Generate validation and testing sequences\n",
    "val_sequences = process_sequences(sentences[s:ss], seq_length_val_test)\n",
    "val_sequences = modify(val_sequences, vocab_size - 2, vocab_size - 3, 0, 0)\n",
    "\n",
    "test_sequences = process_sequences(sentences[ss:], seq_length_val_test)\n",
    "\n",
    "\n",
    "# Convert ot tensor\n",
    "X_train_seq = torch.tensor([seq[:-1] for seq in train_sequences], dtype=torch.long)\n",
    "y_train_seq = torch.tensor([seq[-1] for seq in train_sequences], dtype=torch.long)\n",
    "\n",
    "\n",
    "X_val_seq = torch.tensor([seq[:-1] for seq in val_sequences], dtype=torch.long)\n",
    "y_val_seq = torch.tensor([seq[-1] for seq in val_sequences], dtype=torch.long)\n",
    "\n",
    "X_test_seq = torch.tensor([seq[:-1] for seq in test_sequences], dtype=torch.long)\n",
    "y_test_seq = torch.tensor([seq[-1] for seq in test_sequences], dtype=torch.long)\n",
    "\n",
    "# Split dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train_seq, y_train_seq, test_size=0.3, random_state=42) # Training\n",
    "X_train = X_train_seq\n",
    "y_train = y_train_seq\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test_seq, y_val_test_seq, test_size=0.5, random_state=42)  # Validation and Testing\n",
    "X_val = X_val_seq\n",
    "y_val = y_val_seq\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = {\n",
    "    'patience': 3,\n",
    "    'counter': 0,\n",
    "    'best_val_loss': float('inf')\n",
    "}\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, train_perplexities, val_perplexities = [], [], [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    train_acc = evaluate(model, train_loader, criterion)[1]\n",
    "\n",
    "    train_perplexity = np.exp(train_loss)\n",
    "    val_perplexity = np.exp(val_loss)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    train_perplexities.append(train_perplexity)\n",
    "    val_perplexities.append(val_perplexity)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f} - Train Accuracy: {train_acc:.4f} - Train Perplexity: {train_perplexity:.4f} - Val loss: {val_loss:.4f} - Val Accuracy: {val_acc:.4f} - Val Perplexity: {val_perplexity:.4f}')\n",
    "\n",
    "    if val_loss < early_stopping['best_val_loss']:\n",
    "        early_stopping['best_val_loss'] = val_loss\n",
    "        early_stopping['counter'] = 0\n",
    "    else:\n",
    "        early_stopping['counter'] += 1\n",
    "\n",
    "    if early_stopping['counter'] >= early_stopping['patience']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_perplexities, label='Train Perplexity')\n",
    "plt.plot(val_perplexities, label='Val Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 mask, 0 misspell\n",
    "test_sequences = process_sequences(sentences[ss:], seq_length_val_test)\n",
    "# test_sequences = modify(test_sequences, vocab_size - 2, vocab_size - 3, 0.1, 0.1)\n",
    "\n",
    "X_test_seq = torch.tensor([seq[:-1] for seq in test_sequences], dtype=torch.long)\n",
    "y_test_seq = torch.tensor([seq[-1] for seq in test_sequences], dtype=torch.long)\n",
    "\n",
    "X_test = X_test_seq\n",
    "y_test = y_test_seq\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 mask, 0 misspell\n",
    "test_sequences = process_sequences(sentences[ss:], seq_length_val_test)\n",
    "test_sequences = modify(test_sequences, vocab_size - 2, vocab_size - 3, 0.05, 0)\n",
    "\n",
    "X_test_seq = torch.tensor([seq[:-1] for seq in test_sequences], dtype=torch.long)\n",
    "y_test_seq = torch.tensor([seq[-1] for seq in test_sequences], dtype=torch.long)\n",
    "\n",
    "X_test = X_test_seq\n",
    "y_test = y_test_seq\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 mask, 0 misspell\n",
    "test_sequences = process_sequences(sentences[ss:], seq_length_val_test)\n",
    "test_sequences = modify(test_sequences, vocab_size - 2, vocab_size - 3, 0.1, 0)\n",
    "\n",
    "X_test_seq = torch.tensor([seq[:-1] for seq in test_sequences], dtype=torch.long)\n",
    "y_test_seq = torch.tensor([seq[-1] for seq in test_sequences], dtype=torch.long)\n",
    "\n",
    "X_test = X_test_seq\n",
    "y_test = y_test_seq\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = process_sequences(sentences[ss:], seq_length_val_test)\n",
    "test_sequences = modify(test_sequences, vocab_size - 2, vocab_size - 3, 0.2, 0)\n",
    "\n",
    "X_test_seq = torch.tensor([seq[:-1] for seq in test_sequences], dtype=torch.long)\n",
    "y_test_seq = torch.tensor([seq[-1] for seq in test_sequences], dtype=torch.long)\n",
    "\n",
    "X_test = X_test_seq\n",
    "y_test = y_test_seq\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = process_sequences(sentences[ss:], seq_length_val_test)\n",
    "test_sequences = modify(test_sequences, vocab_size - 2, vocab_size - 3, 0, 0.05)\n",
    "\n",
    "X_test_seq = torch.tensor([seq[:-1] for seq in test_sequences], dtype=torch.long)\n",
    "y_test_seq = torch.tensor([seq[-1] for seq in test_sequences], dtype=torch.long)\n",
    "\n",
    "X_test = X_test_seq\n",
    "y_test = y_test_seq\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = process_sequences(sentences[ss:], seq_length_val_test)\n",
    "test_sequences = modify(test_sequences, vocab_size - 2, vocab_size - 3, 0, 0.10)\n",
    "\n",
    "X_test_seq = torch.tensor([seq[:-1] for seq in test_sequences], dtype=torch.long)\n",
    "y_test_seq = torch.tensor([seq[-1] for seq in test_sequences], dtype=torch.long)\n",
    "\n",
    "X_test = X_test_seq\n",
    "y_test = y_test_seq\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = process_sequences(sentences[ss:], seq_length_val_test)\n",
    "test_sequences = modify(test_sequences, vocab_size - 2, vocab_size - 3, 0, 0.2)\n",
    "\n",
    "X_test_seq = torch.tensor([seq[:-1] for seq in test_sequences], dtype=torch.long)\n",
    "y_test_seq = torch.tensor([seq[-1] for seq in test_sequences], dtype=torch.long)\n",
    "\n",
    "X_test = X_test_seq\n",
    "y_test = y_test_seq\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = process_sequences(sentences[ss:], seq_length_val_test)\n",
    "test_sequences = modify(test_sequences, vocab_size - 2, vocab_size - 3, 0.05, 0.05)\n",
    "\n",
    "X_test_seq = torch.tensor([seq[:-1] for seq in test_sequences], dtype=torch.long)\n",
    "y_test_seq = torch.tensor([seq[-1] for seq in test_sequences], dtype=torch.long)\n",
    "\n",
    "X_test = X_test_seq\n",
    "y_test = y_test_seq\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask Words (Prob = 5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Split sentences into words\n",
    "words = word_tokenize(text)\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "vocab = Counter(words)\n",
    "\n",
    "# Add <pad>\n",
    "pad_token = '<pad>'\n",
    "mask_token = '<mask>'\n",
    "vocab_size = len(vocab) + 2\n",
    "\n",
    "word_to_idx = {word: i for i, (word, _) in enumerate(vocab.items())}\n",
    "word_to_idx[pad_token] = vocab_size - 1  # '<pad>'\n",
    "word_to_idx[mask_token] = vocab_size - 2 # '<mask>'\n",
    "\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "def words_to_indices(sequence):\n",
    "    return [word_to_idx.get(word, word_to_idx[pad_token]) for word in sequence]\n",
    "\n",
    "\n",
    "def process_sequences(sentences, seq_length):\n",
    "    sequences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [word.lower() for word in words if word.isalpha()]\n",
    "        if len(words) < seq_length:\n",
    "            words.extend([pad_token] * (seq_length - len(words)))\n",
    "        seqs = [words[i:i + seq_length] for i in range(len(words) - seq_length + 1)]\n",
    "        sequences.extend(seqs)\n",
    "    return [words_to_indices(seq) for seq in sequences]\n",
    "\n",
    "\n",
    "def mask_words_in_training(sequences, mask_token=vocab_size - 2, mask_prob=0.05):\n",
    "    masked_sequences = []\n",
    "    for seq in sequences:\n",
    "        masked_seq = [mask_token if random.random() < mask_prob else word for word in seq]\n",
    "        masked_sequences.append(masked_seq)\n",
    "    return masked_sequences\n",
    "\n",
    "# Set sequence length\n",
    "seq_length_train = 10  # Sequence size for training\n",
    "seq_length_val_test = 5  # Sequence size for validation and testing\n",
    "\n",
    "# Generate training sequences\n",
    "train_sequences = process_sequences(sentences, seq_length_train)\n",
    "\n",
    "# New\n",
    "train_sequences = mask_words_in_training(train_sequences)\n",
    "\n",
    "# Generate validation and testing sequences\n",
    "val_test_sequences = process_sequences(sentences, seq_length_val_test)\n",
    "\n",
    "# Convert ot tensor\n",
    "X_train_seq = torch.tensor([seq[:-1] for seq in train_sequences], dtype=torch.long)\n",
    "y_train_seq = torch.tensor([seq[-1] for seq in train_sequences], dtype=torch.long)\n",
    "\n",
    "X_val_test_seq = torch.tensor([seq[:-1] for seq in val_test_sequences], dtype=torch.long)\n",
    "y_val_test_seq = torch.tensor([seq[-1] for seq in val_test_sequences], dtype=torch.long)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_seq, y_train_seq, test_size=0.3, random_state=42) # Training\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test_seq, y_val_test_seq, test_size=0.5, random_state=42)  # Validation and Testing\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize LSTM Model\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early Stoppin\n",
    "\n",
    "early_stopping = {\n",
    "    'patience': 2,  # was 3\n",
    "    'counter': 0,\n",
    "    'best_val_loss': float('inf')\n",
    "}\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, train_perplexities, val_perplexities = [], [], [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    train_acc = evaluate(model, train_loader, criterion)[1]\n",
    "\n",
    "    train_perplexity = np.exp(train_loss)\n",
    "    val_perplexity = np.exp(val_loss)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    train_perplexities.append(train_perplexity)\n",
    "    val_perplexities.append(val_perplexity)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f} - Train Accuracy: {train_acc:.4f} - Train Perplexity: {train_perplexity:.4f} - Val loss: {val_loss:.4f} - Val Accuracy: {val_acc:.4f} - Val Perplexity: {val_perplexity:.4f}')\n",
    "\n",
    "    if val_loss < early_stopping['best_val_loss']:\n",
    "        early_stopping['best_val_loss'] = val_loss\n",
    "        early_stopping['counter'] = 0\n",
    "    else:\n",
    "        early_stopping['counter'] += 1\n",
    "\n",
    "    if early_stopping['counter'] >= early_stopping['patience']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_perplexities, label='Train Perplexity')\n",
    "plt.plot(val_perplexities, label='Val Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask Words (Prob = 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Split sentences into words\n",
    "words = word_tokenize(text)\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "vocab = Counter(words)\n",
    "\n",
    "# Add <pad>\n",
    "pad_token = '<pad>'\n",
    "mask_token = '<mask>'\n",
    "vocab_size = len(vocab) + 2\n",
    "\n",
    "word_to_idx = {word: i for i, (word, _) in enumerate(vocab.items())}\n",
    "word_to_idx[pad_token] = vocab_size - 1  # '<pad>'\n",
    "word_to_idx[mask_token] = vocab_size - 2 # '<mask>'\n",
    "\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "def words_to_indices(sequence):\n",
    "    return [word_to_idx.get(word, word_to_idx[pad_token]) for word in sequence]\n",
    "\n",
    "\n",
    "def process_sequences(sentences, seq_length):\n",
    "    sequences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [word.lower() for word in words if word.isalpha()]\n",
    "        if len(words) < seq_length:\n",
    "            words.extend([pad_token] * (seq_length - len(words)))\n",
    "        seqs = [words[i:i + seq_length] for i in range(len(words) - seq_length + 1)]\n",
    "        sequences.extend(seqs)\n",
    "    return [words_to_indices(seq) for seq in sequences]\n",
    "\n",
    "\n",
    "def mask_words_in_training(sequences, mask_token=vocab_size - 2, mask_prob=0.1):\n",
    "    masked_sequences = []\n",
    "    for seq in sequences:\n",
    "        masked_seq = [mask_token if random.random() < mask_prob else word for word in seq]\n",
    "        masked_sequences.append(masked_seq)\n",
    "    return masked_sequences\n",
    "\n",
    "# Set sequence length\n",
    "seq_length_train = 10  # Sequence size for training\n",
    "seq_length_val_test = 5  # Sequence size for validation and testing\n",
    "\n",
    "# Generate training sequences\n",
    "train_sequences = process_sequences(sentences, seq_length_train)\n",
    "\n",
    "# New\n",
    "train_sequences = mask_words_in_training(train_sequences)\n",
    "\n",
    "# Generate validation and testing sequences\n",
    "val_test_sequences = process_sequences(sentences, seq_length_val_test)\n",
    "\n",
    "# Convert ot tensor\n",
    "X_train_seq = torch.tensor([seq[:-1] for seq in train_sequences], dtype=torch.long)\n",
    "y_train_seq = torch.tensor([seq[-1] for seq in train_sequences], dtype=torch.long)\n",
    "\n",
    "X_val_test_seq = torch.tensor([seq[:-1] for seq in val_test_sequences], dtype=torch.long)\n",
    "y_val_test_seq = torch.tensor([seq[-1] for seq in val_test_sequences], dtype=torch.long)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_seq, y_train_seq, test_size=0.3, random_state=42) # Training\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test_seq, y_val_test_seq, test_size=0.5, random_state=42)  # Validation and Testing\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize LSTM Model\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early Stoppin\n",
    "\n",
    "early_stopping = {\n",
    "    'patience': 2,  # was 3\n",
    "    'counter': 0,\n",
    "    'best_val_loss': float('inf')\n",
    "}\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, train_perplexities, val_perplexities = [], [], [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    train_acc = evaluate(model, train_loader, criterion)[1]\n",
    "\n",
    "    train_perplexity = np.exp(train_loss)\n",
    "    val_perplexity = np.exp(val_loss)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    train_perplexities.append(train_perplexity)\n",
    "    val_perplexities.append(val_perplexity)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f} - Train Accuracy: {train_acc:.4f} - Train Perplexity: {train_perplexity:.4f} - Val loss: {val_loss:.4f} - Val Accuracy: {val_acc:.4f} - Val Perplexity: {val_perplexity:.4f}')\n",
    "\n",
    "    if val_loss < early_stopping['best_val_loss']:\n",
    "        early_stopping['best_val_loss'] = val_loss\n",
    "        early_stopping['counter'] = 0\n",
    "    else:\n",
    "        early_stopping['counter'] += 1\n",
    "\n",
    "    if early_stopping['counter'] >= early_stopping['patience']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_perplexities, label='Train Perplexity')\n",
    "plt.plot(val_perplexities, label='Val Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask Words (Prob = 15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Split sentences into words\n",
    "words = word_tokenize(text)\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "vocab = Counter(words)\n",
    "\n",
    "# Add <pad>\n",
    "pad_token = '<pad>'\n",
    "mask_token = '<mask>'\n",
    "vocab_size = len(vocab) + 2\n",
    "\n",
    "word_to_idx = {word: i for i, (word, _) in enumerate(vocab.items())}\n",
    "word_to_idx[pad_token] = vocab_size - 1  # '<pad>'\n",
    "word_to_idx[mask_token] = vocab_size - 2 # '<mask>'\n",
    "\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "def words_to_indices(sequence):\n",
    "    return [word_to_idx.get(word, word_to_idx[pad_token]) for word in sequence]\n",
    "\n",
    "\n",
    "def process_sequences(sentences, seq_length):\n",
    "    sequences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [word.lower() for word in words if word.isalpha()]\n",
    "        if len(words) < seq_length:\n",
    "            words.extend([pad_token] * (seq_length - len(words)))\n",
    "        seqs = [words[i:i + seq_length] for i in range(len(words) - seq_length + 1)]\n",
    "        sequences.extend(seqs)\n",
    "    return [words_to_indices(seq) for seq in sequences]\n",
    "\n",
    "\n",
    "def mask_words_in_training(sequences, mask_token=vocab_size - 2, mask_prob=0):\n",
    "    masked_sequences = []\n",
    "    for seq in sequences:\n",
    "        masked_seq = [mask_token if random.random() < mask_prob else word for word in seq]\n",
    "        masked_sequences.append(masked_seq)\n",
    "    return masked_sequences\n",
    "\n",
    "# Set sequence length\n",
    "seq_length_train = 20  # Sequence size for training\n",
    "seq_length_val_test = 5  # Sequence size for validation and testing\n",
    "\n",
    "# Generate training sequences\n",
    "train_sequences = process_sequences(sentences, seq_length_train)\n",
    "\n",
    "# New\n",
    "train_sequences = mask_words_in_training(train_sequences)\n",
    "\n",
    "# Generate validation and testing sequences\n",
    "val_test_sequences = process_sequences(sentences, seq_length_val_test)\n",
    "\n",
    "# Convert ot tensor\n",
    "X_train_seq = torch.tensor([seq[:-1] for seq in train_sequences], dtype=torch.long)\n",
    "y_train_seq = torch.tensor([seq[-1] for seq in train_sequences], dtype=torch.long)\n",
    "\n",
    "X_val_test_seq = torch.tensor([seq[:-1] for seq in val_test_sequences], dtype=torch.long)\n",
    "y_val_test_seq = torch.tensor([seq[-1] for seq in val_test_sequences], dtype=torch.long)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_seq, y_train_seq, test_size=0.3, random_state=42) # Training\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test_seq, y_val_test_seq, test_size=0.5, random_state=42)  # Validation and Testing\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize LSTM Model\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early Stoppin\n",
    "\n",
    "early_stopping = {\n",
    "    'patience': 2,  # was 3\n",
    "    'counter': 0,\n",
    "    'best_val_loss': float('inf')\n",
    "}\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, train_perplexities, val_perplexities = [], [], [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    train_acc = evaluate(model, train_loader, criterion)[1]\n",
    "\n",
    "    train_perplexity = np.exp(train_loss)\n",
    "    val_perplexity = np.exp(val_loss)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    train_perplexities.append(train_perplexity)\n",
    "    val_perplexities.append(val_perplexity)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f} - Train Accuracy: {train_acc:.4f} - Train Perplexity: {train_perplexity:.4f} - Val loss: {val_loss:.4f} - Val Accuracy: {val_acc:.4f} - Val Perplexity: {val_perplexity:.4f}')\n",
    "\n",
    "    if val_loss < early_stopping['best_val_loss']:\n",
    "        early_stopping['best_val_loss'] = val_loss\n",
    "        early_stopping['counter'] = 0\n",
    "    else:\n",
    "        early_stopping['counter'] += 1\n",
    "\n",
    "    if early_stopping['counter'] >= early_stopping['patience']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_perplexities, label='Train Perplexity')\n",
    "plt.plot(val_perplexities, label='Val Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Split sentences into words\n",
    "words = word_tokenize(text)\n",
    "words = [word.lower() for word in words if word.isalpha()]\n",
    "vocab = Counter(words)\n",
    "\n",
    "# Add <pad>\n",
    "pad_token = '<pad>'\n",
    "mask_token = '<mask>'\n",
    "vocab_size = len(vocab) + 2\n",
    "\n",
    "word_to_idx = {word: i for i, (word, _) in enumerate(vocab.items())}\n",
    "word_to_idx[pad_token] = vocab_size - 1  # '<pad>'\n",
    "word_to_idx[mask_token] = vocab_size - 2 # '<mask>'\n",
    "\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "def words_to_indices(sequence):\n",
    "    return [word_to_idx.get(word, word_to_idx[pad_token]) for word in sequence]\n",
    "\n",
    "\n",
    "def process_sequences(sentences, seq_length):\n",
    "    sequences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [word.lower() for word in words if word.isalpha()]\n",
    "        if len(words) < seq_length:\n",
    "            words.extend([pad_token] * (seq_length - len(words)))\n",
    "        seqs = [words[i:i + seq_length] for i in range(len(words) - seq_length + 1)]\n",
    "        sequences.extend(seqs)\n",
    "    return [words_to_indices(seq) for seq in sequences]\n",
    "\n",
    "\n",
    "def mask_words_in_training(sequences, mask_token=vocab_size - 2, mask_prob=0):\n",
    "    masked_sequences = []\n",
    "    for seq in sequences:\n",
    "        masked_seq = [mask_token if random.random() < mask_prob else word for word in seq]\n",
    "        masked_sequences.append(masked_seq)\n",
    "    return masked_sequences\n",
    "\n",
    "# Set sequence length\n",
    "seq_length_train = 5  # Sequence size for training\n",
    "seq_length_val_test = 5  # Sequence size for validation and testing\n",
    "\n",
    "s = int(0.7 * len(sentences))\n",
    "# Generate training sequences\n",
    "train_sequences = process_sequences(sentences[:s], seq_length_train)\n",
    "\n",
    "# New\n",
    "train_sequences = mask_words_in_training(train_sequences)\n",
    "\n",
    "# Generate validation and testing sequences\n",
    "val_test_sequences = process_sequences(sentences[s:], seq_length_val_test)\n",
    "\n",
    "# Convert ot tensor\n",
    "X_train_seq = torch.tensor([seq[:-1] for seq in train_sequences], dtype=torch.long)\n",
    "y_train_seq = torch.tensor([seq[-1] for seq in train_sequences], dtype=torch.long)\n",
    "\n",
    "X_val_test_seq = torch.tensor([seq[:-1] for seq in val_test_sequences], dtype=torch.long)\n",
    "y_val_test_seq = torch.tensor([seq[-1] for seq in val_test_sequences], dtype=torch.long)\n",
    "\n",
    "# Split dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train_seq, y_train_seq, test_size=0.3, random_state=42) # Training\n",
    "X_train = X_train_seq\n",
    "y_train = y_train_seq\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test_seq, y_val_test_seq, test_size=0.5, random_state=42)  # Validation and Testing\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize LSTM Model\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout).to(device)\n",
    "\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early Stoppin\n",
    "\n",
    "early_stopping = {\n",
    "    'patience': 2,  # was 3\n",
    "    'counter': 0,\n",
    "    'best_val_loss': float('inf')\n",
    "}\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, train_perplexities, val_perplexities = [], [], [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    train_acc = evaluate(model, train_loader, criterion)[1]\n",
    "\n",
    "    train_perplexity = np.exp(train_loss)\n",
    "    val_perplexity = np.exp(val_loss)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    train_perplexities.append(train_perplexity)\n",
    "    val_perplexities.append(val_perplexity)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f} - Train Accuracy: {train_acc:.4f} - Train Perplexity: {train_perplexity:.4f} - Val loss: {val_loss:.4f} - Val Accuracy: {val_acc:.4f} - Val Perplexity: {val_perplexity:.4f}')\n",
    "\n",
    "    if val_loss < early_stopping['best_val_loss']:\n",
    "        early_stopping['best_val_loss'] = val_loss\n",
    "        early_stopping['counter'] = 0\n",
    "    else:\n",
    "        early_stopping['counter'] += 1\n",
    "\n",
    "    if early_stopping['counter'] >= early_stopping['patience']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_perplexities, label='Train Perplexity')\n",
    "plt.plot(val_perplexities, label='Val Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "test_perplexity = np.exp(test_loss)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Perplexity: {test_perplexity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "split_index = int(0.96 * len(words))\n",
    "split_index_mask = int(0.96 * len(noisy_words_mask))\n",
    "split_index_spelling = int(0.96 * len(noisy_words_spelling))\n",
    "\n",
    "test_words = words[split_index:]\n",
    "test_words_mask = words[split_index_mask:]\n",
    "test_words_spelling = words[split_index_spelling:]\n",
    "\n",
    "# Load the pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "def predict_next_word(sequence, model, tokenizer):\n",
    "    input_ids = tokenizer.encode(sequence, return_tensors='tf')\n",
    "    attention_mask = tf.ones(input_ids.shape, dtype=tf.int32)  # Create an attention mask\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=len(input_ids[0]) + 1, num_return_sequences=1)\n",
    "    return tokenizer.decode(output[0][-1], skip_special_tokens=True)\n",
    "\n",
    "n = 5  # Number of words to use for predicting the next one\n",
    "correct_predictions = 0\n",
    "\n",
    "for i in range(n, len(test_words)):\n",
    "    prompt = ' '.join(test_words[i-n:i])\n",
    "    predicted_word = predict_next_word(prompt, model, tokenizer)\n",
    "    if predicted_word.strip().lower() == test_words[i].lower():\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy_GPT2 = correct_predictions / (len(test_words) - n)\n",
    "print(f\"Accuracy of GPT2: {accuracy_GPT2 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
